{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kebakaran"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Page 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Import Libraries\n",
    "- Get URL\n",
    "- Extracts `h2` for title  and `a` for links\n",
    "- Save it to excel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL_1 = \"https://www.cnnindonesia.com/tag/kebakaran\"\n",
    "#URL = \"https://realpython.github.io/fake-jobs/jobs/senior-python-developer-0\"\n",
    "page = requests.get(URL_1)\n",
    "print(page.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(page.content, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = soup.find_all(\"article\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_tags = []\n",
    "for article in articles:\n",
    "    article_tags.append(article)\n",
    "    #print(article, end=\"\\n\"*2)\n",
    "print(article_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_list = []\n",
    "title_list = []\n",
    "for article_tag in article_tags:\n",
    "    a_tag = article_tag.find('a')\n",
    "    h2_tag = article_tag.find('h2')\n",
    "    \n",
    "    if a_tag is not None:\n",
    "        url = a_tag['href']\n",
    "        url_list.append(url)\n",
    "        #print(\"URL:\", url)\n",
    "        \n",
    "    if h2_tag is not None:\n",
    "        title = h2_tag.get_text()\n",
    "        title_list.append(title)\n",
    "        #print(\"Title:\", title)\n",
    "    \n",
    "print(\"URL:\", url_list)\n",
    "print(\"Title:\", title_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    'Judul Berita Kebakaran Halaman 1' : title_list,\n",
    "    'Link Berita': url_list\n",
    "})\n",
    "print(df[24:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[24:].to_excel('kebakaran 1.xlsx', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Page (crashed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"https://www.cnnindonesia.com/tag/kebakaran/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_tags = []\n",
    "url_list = []\n",
    "title_list = []\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'Judul Berita Kebakaran Halaman 2-5' : title_list,\n",
    "    'Link Berita': url_list\n",
    "})\n",
    "\n",
    "articles_per_page = 34\n",
    "num_of_articles = df.shape[0] // articles_per_page\n",
    "\n",
    "for i in range(num_of_articles):\n",
    "    # get thefirst index at article\n",
    "    start_index = i * articles_per_page\n",
    "    # get the last 10 \n",
    "    end_index = start_index + articles_per_page\n",
    "\n",
    "for page_num in range(start_index,end_index):\n",
    "    url = base_url + str(page_num)\n",
    "    # print(url)\n",
    "    \n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    \n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    articles = soup.find_all(\"article\")\n",
    "    \n",
    "    for article in articles:\n",
    "        article_tags.append(article)\n",
    "        \n",
    "        for article_tag in article_tags:\n",
    "            a_tag = article_tag.find('a')\n",
    "            h2_tag = article_tag.find('h2')\n",
    "    \n",
    "            if a_tag is not None:\n",
    "                url = a_tag['href']\n",
    "                url_list.append(url)\n",
    "                #print(\"URL:\", url)\n",
    "                \n",
    "            if h2_tag is not None:\n",
    "                title = h2_tag.get_text()\n",
    "                title_list.append(title)\n",
    "                #print(\"Title:\", title)\n",
    "    \n",
    "#print(\"URL:\", url_list)\n",
    "#print(\"Title:\", title_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"URL:\", url_list)\n",
    "print(\"Title:\", title_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_kebakaran_df = pd.DataFrame()\n",
    "for i in range(num_of_articles):\n",
    "    combine = df[start_index:end_index].tail(10)\n",
    "    combine_kebakaran_df = pd.concat([combine_kebakaran_df, combine])\n",
    "\n",
    "print(combine_kebakaran_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    'Judul Berita Kebakaran Halaman 2-5' : title_list,\n",
    "    'Link Berita': url_list\n",
    "})\n",
    "\n",
    "articles_per_page = 34\n",
    "num_of_articles = df.shape[0] // articles_per_page\n",
    "\n",
    "combine_kebakaran_df = pd.DataFrame()\n",
    "\n",
    "for i in range(num_of_articles):\n",
    "    # get thefirst index at article\n",
    "    start_index = i * articles_per_page\n",
    "    # get the last 10 \n",
    "    end_index = start_index + articles_per_page\n",
    "    \n",
    "    combine = df[start_index:end_index].tail(10)\n",
    "    combine_kebakaran_df = pd.concat([combine_kebakaran_df, combine])\n",
    "\n",
    "print(combine_kebakaran_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_kebakaran_df.to_excel('combined_results.xlsx', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Page 2-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL_5 = \"https://www.cnnindonesia.com/tag/kebakaran/5\"\n",
    "page = requests.get(URL_5)\n",
    "#print(page.text)\n",
    "soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "articles = soup.find_all(\"article\")\n",
    "article_tags = []\n",
    "for article in articles:\n",
    "    article_tags.append(article)\n",
    "    #print(article, end=\"\\n\"*2)\n",
    "#print(article_tags)\n",
    "url_list = []\n",
    "title_list = []\n",
    "for article_tag in article_tags:\n",
    "    a_tag = article_tag.find('a')\n",
    "    h2_tag = article_tag.find('h2')\n",
    "    \n",
    "    if a_tag is not None:\n",
    "        url = a_tag['href']\n",
    "        url_list.append(url)\n",
    "        #print(\"URL:\", url)\n",
    "        \n",
    "    if h2_tag is not None:\n",
    "        title = h2_tag.get_text()\n",
    "        title_list.append(title)\n",
    "        #print(\"Title:\", title)\n",
    "    \n",
    "#print(\"URL:\", url_list)\n",
    "#print(\"Title:\", title_list)\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'Judul Berita Kebakaran Halaman 1' : title_list,\n",
    "    'Link Berita': url_list\n",
    "})\n",
    "#print(df[24:])\n",
    "df[24:].to_excel('kebakaran 5.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = '../ml-capstone/news_titles_urls/kebakaran/'\n",
    "\n",
    "excel_files = glob.glob(dir + '*.xlsx')\n",
    "\n",
    "dfs = []\n",
    "\n",
    "for file in excel_files:\n",
    "    df = pd.read_excel(file)\n",
    "    dfs.append(df) \n",
    "combined_df = pd.concat(dfs, ignore_index=True)\n",
    "combined_df.to_excel('Kebakaran.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.to_excel('Kebakaran.xlsx', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kejahatan (Polisi)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Page 1-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL_5 = \"https://www.cnnindonesia.com/tag/kejahatan/5\"\n",
    "page = requests.get(URL_5)\n",
    "#print(page.text)\n",
    "soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "articles = soup.find_all(\"article\")\n",
    "article_tags = []\n",
    "for article in articles:\n",
    "    article_tags.append(article)\n",
    "    #print(article, end=\"\\n\"*2)\n",
    "#print(article_tags)\n",
    "url_list = []\n",
    "title_list = []\n",
    "for article_tag in article_tags:\n",
    "    a_tag = article_tag.find('a')\n",
    "    h2_tag = article_tag.find('h2')\n",
    "    \n",
    "    if a_tag is not None:\n",
    "        url = a_tag['href']\n",
    "        url_list.append(url)\n",
    "        #print(\"URL:\", url)\n",
    "        \n",
    "    if h2_tag is not None:\n",
    "        title = h2_tag.get_text()\n",
    "        title_list.append(title)\n",
    "        #print(\"Title:\", title)\n",
    "    \n",
    "#print(\"URL:\", url_list)\n",
    "#print(\"Title:\", title_list)\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'Judul Berita Kejahatan' : title_list,\n",
    "    'Link Berita': url_list\n",
    "})\n",
    "#print(df[24:])\n",
    "df[24:].to_excel('Kejatahan 5.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = '../ml-capstone/news_titles_urls/kejahatan/'\n",
    "\n",
    "excel_files = glob.glob(dir + '*.xlsx')\n",
    "\n",
    "dfs = []\n",
    "\n",
    "for file in excel_files:\n",
    "    df = pd.read_excel(file)\n",
    "    dfs.append(df) \n",
    "combined_df = pd.concat(dfs, ignore_index=True)\n",
    "combined_df.to_excel('Kejahatan.xlsx', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
